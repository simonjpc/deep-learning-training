{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a feed forward neural network with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import kagglehub\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/hojjatk/mnist-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22.0M/22.0M [00:03<00:00, 7.21MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simon/.cache/kagglehub/datasets/hojjatk/mnist-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths for the training and testing data\n",
    "train_image = os.path.join(path, 'train-images.idx3-ubyte')  # Path to the training images file\n",
    "train_label = os.path.join(path, 'train-labels.idx1-ubyte')  # Path to the training labels file\n",
    "\n",
    "test_image = os.path.join(path, 't10k-images.idx3-ubyte')  # Path to the test images file\n",
    "test_label = os.path.join(path, 't10k-labels.idx1-ubyte')  # Path to the test labels file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from array import array\n",
    "import numpy as np\n",
    "\n",
    "class MnistDataLoader:\n",
    "    \n",
    "    def __init__(self, train_image, train_label, test_image, test_label):\n",
    "        self.train_image = train_image\n",
    "        self.train_label = train_label\n",
    "        self.test_image = test_image\n",
    "        self.test_label = test_label\n",
    "\n",
    "    def read_images_labels(self, img_path, label_path):\n",
    "\n",
    "        # read images\n",
    "        with open(label_path, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError(f'Magic number mismatch, expected 2049, got {magic}')\n",
    "            labels = np.array(array(\"B\", file.read()))\n",
    "        \n",
    "        # read labels\n",
    "        with open(img_path, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError(f'Magic number mismatch, expected 2051, got {magic}')\n",
    "            image_data = np.array(array(\"B\", file.read())).reshape(size, rows, cols)\n",
    "\n",
    "        return image_data, labels\n",
    "    \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.train_image, self.train_label)\n",
    "        x_test, y_test = self.read_images_labels(self.test_image, self.test_label)\n",
    "\n",
    "        return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = MnistDataLoader(\n",
    "    train_image,\n",
    "    train_label,\n",
    "    test_image,\n",
    "    test_label,\n",
    ")\n",
    "(x_train, y_train), (x_test, y_test) = dataloader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(dataset, batch_size=32):\n",
    "    x_train, y_train = dataset\n",
    "\n",
    "    dataset_batches = []\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        start = cnt * batch_size\n",
    "        end = (cnt + 1) * batch_size\n",
    "        single_input_batch = x_train[start:end]\n",
    "        single_output_batch = y_train[start:end]\n",
    "        dataset_batches.append((single_input_batch, single_output_batch))\n",
    "        if start >= len(x_train) or end >= len(x_train):\n",
    "            break\n",
    "        cnt += 1\n",
    "    return dataset_batches\n",
    "\n",
    "train_batches = create_batches((x_train, y_train))\n",
    "test_batches = create_batches((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model architecture numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, input_size: int = 784, hidden_size: int = 512, output_size: int = 10):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 1e-2\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 1e-2\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def relu_activation(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.z1 = np.dot(x, self.W1) + self.b1\n",
    "        self.a1 = self.relu_activation(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        return self.z2\n",
    "    \n",
    "# test\n",
    "nn = NeuralNetwork()\n",
    "sample_input = np.random.randn(1, 784)\n",
    "logits = nn.forward(sample_input)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_xentropy(prediction, ground_truth):\n",
    "    reg_prediction = np.exp(prediction - np.max(prediction))\n",
    "    softmax = reg_prediction / np.sum(reg_prediction, axis=1, keepdims=True)\n",
    "    loss = - np.sum(ground_truth * np.log(softmax + 1e-9))\n",
    "    return softmax, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x, ground_truth, W2, b2, a2, W1, b1, a1, alpha):\n",
    "\n",
    "    dZ2 = a2 - ground_truth # 1 x output_size\n",
    "    dW2 = np.dot(a1.T, dZ2) # hidden_size x 1 \\cdot 1 x output_size = hidden_size x output_size\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) # 1 x output_size\n",
    "\n",
    "    dA1 = np.dot(dZ2, W2.T) # 1 x output_size \\cdot output_size x hidden_size = 1 x hidden_size\n",
    "    dZ1 = dA1 * (a1 > 0) # 1 x hidden_size\n",
    "    dW1 = np.dot(x.T, dZ1) # input_size x 1 \\cdot 1 x hidden_size = input_size x hidden_size\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) # 1 x hidden_size\n",
    "    \n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_step(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "    return W2, b2, W1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss: 18.660388666293496\n",
      "\taverage test loss: 9.817301256231262\n",
      "avg training loss: 8.780908444943584\n",
      "\taverage test loss: 7.6572189720979855\n",
      "avg training loss: 6.968116733521247\n",
      "\taverage test loss: 6.234337221322157\n",
      "avg training loss: 5.737704256329014\n",
      "\taverage test loss: 5.257933173340119\n",
      "avg training loss: 4.850015461446986\n",
      "\taverage test loss: 4.5640817617804865\n",
      "avg training loss: 4.187423748747253\n",
      "\taverage test loss: 4.0616120757158525\n",
      "avg training loss: 3.6754391777321\n",
      "\taverage test loss: 3.6880161506656166\n",
      "avg training loss: 3.268530342195639\n",
      "\taverage test loss: 3.4063118621479385\n",
      "avg training loss: 2.936017501851357\n",
      "\taverage test loss: 3.183390209780591\n",
      "avg training loss: 2.658184733879685\n",
      "\taverage test loss: 2.9966770547911143\n"
     ]
    }
   ],
   "source": [
    "alpha = 1e-3\n",
    "nb_epochs = 10\n",
    "num_classes = 10\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    avg_train_loss = 0\n",
    "    for xtrain, ytrain in train_batches:\n",
    "\n",
    "        xtrain = xtrain.reshape(-1, 28*28)\n",
    "        xtrain = xtrain / 255\n",
    "        ytrain = np.eye(num_classes)[ytrain]\n",
    "        \n",
    "        logits = nn.forward(xtrain)\n",
    "        softm, loss = categorical_xentropy(logits, ytrain)\n",
    "        avg_train_loss += loss\n",
    "        dW1, db1, dW2, db2 = backprop(xtrain, ytrain, nn.W2, nn.b2, softm, nn.W1, nn.b1, nn.a1, alpha)\n",
    "        nn.W2, nn.b2, nn.W1, nn.b1 = optimizer_step(nn.W1, nn.b1, nn.W2, nn.b2, dW1, db1, dW2, db2, alpha)\n",
    "    \n",
    "    avg_train_loss = avg_train_loss / len(train_batches)\n",
    "    print(f\"avg training loss: {avg_train_loss}\")\n",
    "\n",
    "    avg_test_loss = 0\n",
    "    for xtest, ytest in test_batches:\n",
    "\n",
    "        xtest = xtest.reshape(-1, 28*28)\n",
    "        xtest = xtest / 255\n",
    "        ytest = np.eye(num_classes)[ytest]\n",
    "\n",
    "        test_logits = nn.forward(xtest)\n",
    "        _, test_loss = categorical_xentropy(test_logits, ytest)\n",
    "        avg_test_loss += test_loss\n",
    "    avg_test_loss = avg_test_loss / len(test_batches)\n",
    "    print(f\"\\taverage test loss: {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model architecture torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchNeuralNetwork:\n",
    "\n",
    "    def __init__(self, input_size=784, hidden_size=512, output_size=10):\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.W1 = torch.randn(input_size, hidden_size, dtype=torch.float32) * 0.01\n",
    "        self.b1 = torch.zeros(1, hidden_size, dtype=torch.float32)\n",
    "        \n",
    "        self.W2 = torch.randn(hidden_size, output_size, dtype=torch.float32) * 0.01\n",
    "        self.b2 = torch.zeros(1, output_size, dtype=torch.float32)\n",
    "\n",
    "    def activation(self, x):\n",
    "        return torch.relu(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.z1 = torch.matmul(x, self.W1) + self.b1\n",
    "        self.a1 = self.activation(self.z1)\n",
    "        self.z2 = torch.matmul(self.a1, self.W2) + self.b2\n",
    "        \n",
    "        return self.z2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_xentropy(logits, ground_truth):\n",
    "    reg_logits = torch.exp(logits - torch.max(logits))\n",
    "    softmax = reg_logits / torch.sum(reg_logits, dim=1, keepdim=True)\n",
    "    loss = - torch.sum(ground_truth * torch.log(softmax + 1e-9))\n",
    "    return softmax, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_backprop(x, y, W1, b1, a1, W2, b2, a2):\n",
    "\n",
    "    dZ2 = a2 - y\n",
    "    dW2 = torch.matmul(a1.T, dZ2) # hidden_size x output_sze\n",
    "    db2 = torch.sum(dZ2, dim=0, keepdims=True)\n",
    "\n",
    "    da1 = torch.matmul(dZ2, W2.T) # 1 x  hidden_size\n",
    "    dZ1 = da1 * (a1 > 0) # 1 x hidden_size\n",
    "    dW1 = torch.matmul(x.T, dZ1) # input_size x hidden_size\n",
    "    db1 = torch.sum(dZ1, dim=0, keepdims=True) # 1 x hidden_size\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss: 18.75467872619629\n",
      "\tavg test loss: 9.77198600769043\n",
      "avg training loss: 8.728156089782715\n",
      "\tavg test loss: 7.543931484222412\n",
      "avg training loss: 6.894233703613281\n",
      "\tavg test loss: 6.1518659591674805\n",
      "avg training loss: 5.678622245788574\n",
      "\tavg test loss: 5.215864181518555\n",
      "avg training loss: 4.814214706420898\n",
      "\tavg test loss: 4.5432000160217285\n",
      "avg training loss: 4.1673994064331055\n",
      "\tavg test loss: 4.057517051696777\n",
      "avg training loss: 3.6642508506774902\n",
      "\tavg test loss: 3.686331272125244\n",
      "avg training loss: 3.2620186805725098\n",
      "\tavg test loss: 3.401247024536133\n",
      "avg training loss: 2.931161642074585\n",
      "\tavg test loss: 3.177666425704956\n",
      "avg training loss: 2.654737949371338\n",
      "\tavg test loss: 3.0041232109069824\n"
     ]
    }
   ],
   "source": [
    "torch_nn = TorchNeuralNetwork()\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    avg_train_loss = 0\n",
    "    for xtrain, ytrain in train_batches:\n",
    "\n",
    "        # train dataset pre-processing\n",
    "        xtrain = xtrain.reshape(-1, 28*28)\n",
    "        xtrain = xtrain / 255\n",
    "        ytrain = np.eye(num_classes)[ytrain]\n",
    "        xtrain = torch.tensor(xtrain, dtype=torch.float32)\n",
    "        ytrain = torch.tensor(ytrain, dtype=torch.float32)\n",
    "\n",
    "        logits = torch_nn.forward(xtrain)\n",
    "        softmax, loss = torch_xentropy(logits, ytrain)\n",
    "        dW1, db1, dW2, db2 = torch_backprop(xtrain, ytrain, torch_nn.W1, torch_nn.b1, torch_nn.a1, torch_nn.W2, torch_nn.b2, softmax)\n",
    "        torch_nn.W2, torch_nn.b2, torch_nn.W1, torch_nn.b1 = optimizer_step(torch_nn.W1, torch_nn.b1, torch_nn.W2, torch_nn.b2, dW1, db1, dW2, db2, alpha)\n",
    "\n",
    "        avg_train_loss += loss\n",
    "    avg_train_loss = avg_train_loss / len(train_batches)\n",
    "    print(f\"avg training loss: {avg_train_loss}\")\n",
    "\n",
    "    avg_test_loss = 0\n",
    "    for xtest, ytest in test_batches:\n",
    "\n",
    "        xtest = xtest.reshape(-1, 28*28)\n",
    "        xtest = xtest / 255\n",
    "        ytest = np.eye(num_classes)[ytest]\n",
    "        xtest = torch.tensor(xtest, dtype=torch.float32)\n",
    "        ytest = torch.tensor(ytest, dtype=torch.float32)\n",
    "\n",
    "        logits = torch_nn.forward(xtest)\n",
    "        softmax, test_loss = torch_xentropy(logits, ytest)\n",
    "        \n",
    "        avg_test_loss += test_loss\n",
    "\n",
    "    avg_test_loss = avg_test_loss / len(test_batches)\n",
    "    print(f\"\\tavg test loss: {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for a generic number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class TorchNeuralNetworkGeneric:\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "        self.sizes = sizes\n",
    "        self.parameters = {}\n",
    "        self.activations = {}\n",
    "        for l in range(1, len(sizes)):\n",
    "            self.parameters[f\"W{l}\"] = torch.randn(sizes[l-1], sizes[l], dtype=torch.float32) * 0.01\n",
    "            self.parameters[f\"b{l}\"] = torch.randn(1, sizes[l], dtype=torch.float32)\n",
    "\n",
    "    def activation(self, x):\n",
    "        return torch.relu(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.activations[\"A0\"] = x\n",
    "        for l in range(1, len(self.sizes)):\n",
    "            Z = torch.matmul(self.activations[f\"A{l-1}\"], self.parameters[f\"W{l}\"]) + self.parameters[f\"b{l}\"]\n",
    "            A = self.activation(Z)\n",
    "            self.activations[f\"Z{l}\"], self.activations[f\"A{l}\"] = Z, A\n",
    "\n",
    "        self.activations.pop(f\"A{len(self.sizes) - 1}\")\n",
    "        return Z, self.activations\n",
    "\n",
    "# test\n",
    "nn = TorchNeuralNetworkGeneric([784, 512, 128, 10])\n",
    "sample_input = torch.randn(1, 784, dtype=torch.float32)\n",
    "logits, _ = nn.forward(sample_input)\n",
    "print(logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_backprop_generic(y, softmax, parameters, activations):\n",
    "    \n",
    "    grads = {}\n",
    "\n",
    "    dZ = softmax - y\n",
    "\n",
    "    for l in reversed(range(1, len(parameters) // 2 + 1)):\n",
    "        # print(activations[f\"A{l-1}\"].T.dtype, dZ.dtype)\n",
    "        grads[f\"dW{l}\"] = torch.matmul(activations[f\"A{l-1}\"].T, dZ)\n",
    "        grads[f\"db{l}\"] = torch.sum(dZ, dim=0, keepdim=True)\n",
    "\n",
    "        if l > 1:\n",
    "            dA = torch.matmul(dZ, parameters[f\"W{l}\"].T)\n",
    "            dZ = dA * (activations[f\"A{l-1}\"] > 0)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_optimizer_generic(parameters, grads, alpha):\n",
    "    for l in range(1, len(parameters) // 2 + 1):\n",
    "        parameters[f\"W{l}\"] -= alpha * grads[f\"dW{l}\"]\n",
    "        parameters[f\"b{l}\"] -= alpha * grads[f\"db{l}\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss:  19.74854850769043\n",
      "\tavg test loss:  10.386972427368164\n",
      "avg training loss:  9.35973834991455\n",
      "\tavg test loss:  8.373201370239258\n",
      "avg training loss:  7.700314998626709\n",
      "\tavg test loss:  7.005049705505371\n",
      "avg training loss:  6.455615997314453\n",
      "\tavg test loss:  5.962541103363037\n",
      "avg training loss:  5.526224613189697\n",
      "\tavg test loss:  5.2019877433776855\n",
      "avg training loss:  4.821743011474609\n",
      "\tavg test loss:  4.631757736206055\n",
      "avg training loss:  4.2646660804748535\n",
      "\tavg test loss:  4.198936462402344\n",
      "avg training loss:  3.8180737495422363\n",
      "\tavg test loss:  3.8690662384033203\n",
      "avg training loss:  3.4525017738342285\n",
      "\tavg test loss:  3.6049489974975586\n",
      "avg training loss:  3.149456739425659\n",
      "\tavg test loss:  3.395559787750244\n"
     ]
    }
   ],
   "source": [
    "torch_nn_generic = TorchNeuralNetworkGeneric([784, 256, 10])\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    avg_train_loss = 0\n",
    "    for xtrain, ytrain in train_batches:\n",
    "\n",
    "        #train set preprocessing\n",
    "        xtrain = xtrain.reshape(-1, 784)\n",
    "        xtrain = xtrain / 255\n",
    "        xtrain = torch.tensor(xtrain, dtype=torch.float32)\n",
    "        ytrain = np.eye(num_classes)[ytrain]\n",
    "        ytrain = torch.tensor(ytrain, dtype=torch.float32)\n",
    "\n",
    "        logits, _ = torch_nn_generic.forward(xtrain)\n",
    "        softmax, loss = torch_xentropy(logits, ytrain)\n",
    "        avg_train_loss += loss\n",
    "        grads = torch_backprop_generic(ytrain, softmax, torch_nn_generic.parameters, torch_nn_generic.activations)\n",
    "        torch_nn_generic.parameters = torch_optimizer_generic(torch_nn_generic.parameters, grads, alpha)\n",
    "    \n",
    "    avg_train_loss = avg_train_loss / len(train_batches)\n",
    "    print(f\"avg training loss:  {avg_train_loss}\")\n",
    "\n",
    "    avg_test_loss = 0\n",
    "    for xtest, ytest in test_batches:\n",
    "\n",
    "        #test set preprocessing\n",
    "        xtest = xtest.reshape(-1, 784)\n",
    "        xtest = xtest / 255\n",
    "        xtest = torch.tensor(xtest, dtype=torch.float32)\n",
    "        ytest = np.eye(num_classes)[ytest]\n",
    "        ytest = torch.tensor(ytest, dtype=torch.float32)\n",
    "\n",
    "        logits, _ = torch_nn_generic.forward(xtest)\n",
    "        softmax, loss = torch_xentropy(logits, ytest)\n",
    "        avg_test_loss += loss\n",
    "\n",
    "    avg_test_loss = avg_test_loss / len(test_batches)\n",
    "    print(f\"\\tavg test loss:  {avg_test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
