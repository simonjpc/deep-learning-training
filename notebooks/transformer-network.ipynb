{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simon/Documents/venvs/dl_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 9240994/9240994 [00:04<00:00, 2260670.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"alpindale/light-novels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = \"\\n\".join(dataset[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char based tokenization and not BPE (for simplicity)\n",
    "vocab = list(set(text_dataset))\n",
    "encoder_map = {v: i for i, v in enumerate(vocab)}\n",
    "decoder_map = {i: v for i, v in enumerate(vocab)}\n",
    "\n",
    "encode = lambda sentence: [encoder_map[character] for character in sentence]\n",
    "decode = lambda lofints: \"\".join([decoder_map[i] for i in lofints])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.randn((100, 32), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3904,  0.1600, -0.0364,  0.7662, -2.0117,  0.2296,  1.9901, -1.1554,\n",
       "        -1.3125,  0.8000,  1.6572,  0.1503,  0.6569,  0.0314, -1.1379,  0.1635,\n",
       "        -1.0883, -0.1894,  0.8328, -0.1696,  0.2562, -0.2693, -1.0407, -0.2597,\n",
       "        -0.9397, -1.2543, -0.2378, -1.8291, -1.8532,  0.7988,  0.2974, -0.1380])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTransformerNetwork:\n",
    "\n",
    "    def __init__(self, vocab_size, model_dim, nb_heads, attn_size):\n",
    "        \n",
    "        # model vector sizes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_dim = model_dim\n",
    "        self.nb_heads = nb_heads\n",
    "\n",
    "        # layers\n",
    "        self.attn_projs = {}\n",
    "        for n in range(self.nb_heads):\n",
    "            self.attn_projs[f\"k_proj_{n}\"] = torch.randn((model_dim, attn_size), dtype=torch.float32)\n",
    "            self.attn_projs[f\"q_proj_{n}\"] = torch.randn((model_dim, attn_size), dtype=torch.float32)\n",
    "            self.attn_projs[f\"v_proj_{n}\"] = torch.randn((model_dim, attn_size), dtype=torch.float32)\n",
    "        self.embedding = torch.randn((vocab_size, model_dim), dtype=torch.float32)\n",
    "        self.attn_proj_back = torch.randn((attn_size * nb_heads, model_dim), dtype=torch.float32)\n",
    "        self.ff_layer = torch.randn((model_dim, model_dim), dtype=torch.float32)\n",
    "        self.ff_layer2 = torch.randn((model_dim, model_dim), dtype=torch.float32)\n",
    "        self.ff_bias = torch.randn((1, model_dim), dtype=torch.float32)\n",
    "        self.ff_bias2 = torch.randn((1, model_dim), dtype=torch.float32)\n",
    "        self.output = torch.randn((model_dim, vocab_size), dtype=torch.float32)\n",
    "\n",
    "        # parameters and activations tracker\n",
    "        self.parameters = {}\n",
    "        self.cache = {}\n",
    "        self.activations = {}\n",
    "\n",
    "    def positional_encoding(self, x_pos):\n",
    "        \"\"\"sinus non-trainable positional encoding to add with input\"\"\"\n",
    "        seq_len = len(x_pos)\n",
    "        positions = x_pos.unsqueeze(1)\n",
    "        div_term = torch.arange(0, self.model_dim, 2) * (- torch.log(torch.tensor(1e4)) / self.model_dim)\n",
    "        pe = torch.zeros(seq_len, self.model_dim)\n",
    "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
    "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "        return pe\n",
    "    \n",
    "    def residual_connexion(self, current_layer, dragged_layer):\n",
    "        \"\"\"generic residual connexion operation\"\"\"\n",
    "        return current_layer + dragged_layer\n",
    "\n",
    "    def input_block(self, x, x_pos):\n",
    "        \"\"\"embedding input and positional encoding input\"\"\"\n",
    "        input_embed = self.embedding[x]\n",
    "        position_embed = self.positional_encoding(x_pos)\n",
    "        total_input = input_embed + position_embed\n",
    "        total_input, total_input_mean, total_input_std = self.layernorm(total_input)\n",
    "        total_input = self.dropout(total_input, 0.2)\n",
    "        return total_input\n",
    "    \n",
    "    def attention_block(self, x):\n",
    "        \"\"\"full attention block\"\"\"\n",
    "        all_attn_outputs = []\n",
    "        for n in range(self.nb_heads):\n",
    "            K, Q, V = (\n",
    "                x @ self.attn_projs[f\"k_proj_{n}\"],\n",
    "                x @ self.attn_projs[f\"q_proj_{n}\"],\n",
    "                x @ self.attn_projs[f\"v_proj_{n}\"]\n",
    "            )\n",
    "            pre_attention = Q @ K.T\n",
    "            scale = pre_attention / torch.sqrt(torch.tensor(self.model_dim, dtype=torch.float32))\n",
    "            ones = torch.ones(scale.size(), dtype=torch.float32)\n",
    "            mask = torch.tril(ones)\n",
    "            masked_scale = scale * mask\n",
    "            masked_scale = masked_scale.masked_fill(mask == 0, float(\"-inf\"))\n",
    "            attn_softm = torch.softmax(masked_scale, dim=1)\n",
    "            attn_softm = self.dropout(attn_softm, 0.2)\n",
    "            attn_output = attn_softm @ V\n",
    "            all_attn_outputs.append(attn_output)\n",
    "        all_attn_outputs = torch.cat(all_attn_outputs, dim=-1)\n",
    "        all_attn_outputs = all_attn_outputs @ self.attn_proj_back\n",
    "        all_attn_outputs = self.dropout(all_attn_outputs, 0.2)\n",
    "        return all_attn_outputs\n",
    "\n",
    "    def linear_block(self, x):\n",
    "        \"\"\"linear layer block before output\"\"\"\n",
    "        self.cache[\"linear_block_input\"] = x\n",
    "        x, x_mean, x_std = self.layernorm(x)\n",
    "        self.cache[\"linear_block_normalized_input\"] = x\n",
    "        self.cache[\"linear_block_mean\"] = x_mean\n",
    "        self.cache[\"linear_block_std\"] = x_std\n",
    "        ff_outout = x @ self.ff_layer + self.ff_bias # a0 * W1 + b1\n",
    "        ff_outout = self.dropout(ff_outout, 0.2) # \n",
    "        ff_activation = self.relu_activation(ff_outout) # a1 = activation(W1 * x + b1)\n",
    "        ff_output2 = ff_activation @ self.ff_layer2 + self.ff_bias2 # a1 * W2  + b2\n",
    "        logits = ff_output2 @ self.embedding.T\n",
    "        return logits\n",
    "    \n",
    "    def dropout(self, x, p):\n",
    "        mask = (torch.rand_like(x) > p).float()\n",
    "        return x * mask\n",
    "    \n",
    "    def layernorm(self, x):\n",
    "        \"\"\"generic normalization layer\"\"\"\n",
    "        x_mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        x_std = torch.std(x, dim=1, keepdim=True)\n",
    "        return (x - x_mean) / (x_std + 1e-6), x_mean, x_std\n",
    "    \n",
    "    def relu_activation(self, x):\n",
    "        \"\"\"generic rectified linear unit layer\"\"\"\n",
    "        return torch.relu(x)\n",
    "\n",
    "    def forward(self, x, x_idx):\n",
    "        \"\"\"full transformer forward pass\"\"\"\n",
    "        total_input = self.input_block(x, x_idx)\n",
    "        attention = self.attention_block(total_input)\n",
    "        residual1 = self.residual_connexion(attention, total_input)\n",
    "        logits = self.linear_block(residual1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, y):\n",
    "    exp_logits = torch.exp(logits - torch.max(logits))\n",
    "    softmax = exp_logits / torch.sum(exp_logits, dim=-1, keepdim=True)\n",
    "    loss = - torch.log(softmax.gather(1, y) + 1e-9).mean() \n",
    "    return softmax, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layernorm_backprop(dout, x, mean, std):\n",
    "\n",
    "    xmu = x - mean\n",
    "    xsgma = 1 / (std + 1e-6)\n",
    "    dnorm = dout * xsgma\n",
    "\n",
    "    dx = dnorm\n",
    "    dx -= dnorm.mean(dim=1, keepdims=1)\n",
    "    dx -= (xmu / (std + 1e-6) ** 2) * (dnorm * xmu)/mean(dim=1, keepdims=1)\n",
    "    return dx\n",
    "\n",
    "def linear_block_backprop(y, output_softmax, parameters, activations, cache):\n",
    "\n",
    "    grads = {}\n",
    "    dZ2 = output_softmax - output_softmax.gather(1, y)\n",
    "\n",
    "    grads[\"dW2\"] = torch.matmul(activations[f\"A1\"].T, dZ2)\n",
    "    grads[\"db2\"] = torch.sum(dZ2, dim=0, keepdim=True)\n",
    "\n",
    "    dA1 = torch.matmul(dZ2, parameters[f\"W2\"].T)\n",
    "    dZ1 = dA1 * (activations[\"A0\"] > 0) # this is the input of the linear block\n",
    "\n",
    "    grads[\"dW1\"] = torch.matmul(activations[\"A0\"].T, dZ1)\n",
    "    grads[\"db1\"] = torch.sum(dZ1, dim=0, keepdim=True)\n",
    "\n",
    "    dA0 = torch.matmul(dZ1, parameters[\"W1\"].T)\n",
    "\n",
    "    grads[\"dA0\"] = layernorm_backprop(\n",
    "        dA0,\n",
    "        cache[\"linear_block_input\"],\n",
    "        cache[\"linear_block_mean\"],\n",
    "        cache[\"linear_block_std\"],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_step():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim, nb_heads, attn_size = 64, 1, 32\n",
    "transformer = BasicTransformerNetwork(vocab_size=len(vocab), model_dim=model_dim, nb_heads=nb_heads, attn_size=attn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"This is an example sentence\"\n",
    "input_ids = encode(example)\n",
    "pos_ids = list(range(len(input_ids)))\n",
    "\n",
    "input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "pos_ids = torch.tensor(pos_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 3165])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.forward(input_ids, pos_ids).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn((27, 64), dtype=torch.float32)\n",
    "y = torch.randint(0, 63, (1, 27), dtype=torch.long)\n",
    "prediction = torch.tensor(30, dtype=torch.float32)\n",
    "exp_logits = torch.exp(logits - torch.max(logits))\n",
    "softmax = exp_logits / torch.sum(exp_logits, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 27]), torch.Size([1, 1, 27]), torch.Size([27, 64]))"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size(), y.unsqueeze(1).size(), softmax.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4],\n",
       "        [58],\n",
       "        [17],\n",
       "        [61],\n",
       "        [37],\n",
       "        [54],\n",
       "        [46],\n",
       "        [ 8],\n",
       "        [ 1],\n",
       "        [39],\n",
       "        [46],\n",
       "        [ 4],\n",
       "        [12],\n",
       "        [47],\n",
       "        [ 0],\n",
       "        [32],\n",
       "        [42],\n",
       "        [ 5],\n",
       "        [43],\n",
       "        [54],\n",
       "        [53],\n",
       "        [58],\n",
       "        [26],\n",
       "        [44],\n",
       "        [21],\n",
       "        [ 0],\n",
       "        [ 7]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.gather(1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
