{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simon/Documents/venvs/dl_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"alpindale/light-novels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = \"\\n\".join(dataset[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char based tokenization and not BPE (for simplicity)\n",
    "vocab = list(set(text_dataset))\n",
    "encoder_map = {v: i for i, v in enumerate(vocab)}\n",
    "decoder_map = {i: v for i, v in enumerate(vocab)}\n",
    "\n",
    "encode = lambda sentence: [encoder_map[character] for character in sentence]\n",
    "decode = lambda lofints: \"\".join([decoder_map[i] for i in lofints])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.randn((100, 32), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7096, -0.1055,  1.2257,  0.1381,  0.0286, -0.7578, -0.1929,  0.2747,\n",
       "         0.2825, -0.5817,  0.2672,  0.7243, -0.2106, -1.9916,  1.7362,  1.8098,\n",
       "        -1.4643,  0.6058,  1.2720,  0.0305, -0.0819, -1.4450,  0.6982, -1.7524,\n",
       "        -0.3701, -0.7441,  1.8367, -0.9371, -0.1402,  0.6811, -0.8513,  1.9684])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTransformerNetwork:\n",
    "\n",
    "    def __init__(self, vocab_size, model_dim, nb_heads):\n",
    "        \n",
    "        # parameters and activations tracker\n",
    "        self.parameters = {\"nb_heads\": nb_heads}\n",
    "        self.cache = {\"dropout_p\": 0.2, \"model_dim\": model_dim, \"nb_heads\": nb_heads}\n",
    "        self.activations = {}\n",
    "\n",
    "        # model vector sizes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_dim = model_dim\n",
    "        self.nb_heads = nb_heads\n",
    "\n",
    "        # layers\n",
    "        self.attn_projs = {}\n",
    "        attn_size = model_dim // nb_heads\n",
    "        for n in range(self.nb_heads):\n",
    "            self.attn_projs[f\"k_proj_{n}\"] = self.init_weights((model_dim, attn_size)) #torch.randn((model_dim, attn_size), dtype=torch.float32) * 0.01\n",
    "            self.attn_projs[f\"q_proj_{n}\"] = self.init_weights((model_dim, attn_size)) #torch.randn((model_dim, attn_size), dtype=torch.float32) * 0.01\n",
    "            self.attn_projs[f\"v_proj_{n}\"] = self.init_weights((model_dim, attn_size)) #torch.randn((model_dim, attn_size), dtype=torch.float32) * 0.01\n",
    "        self.embedding = torch.randn((vocab_size, model_dim), dtype=torch.float32) * 0.01\n",
    "        self.parameters[\"embedding\"] = self.embedding\n",
    "        self.attn_proj_back = self.init_weights((attn_size * nb_heads, model_dim)) #torch.randn((attn_size * nb_heads, model_dim), dtype=torch.float32)\n",
    "        self.ff_layer = self.init_weights((model_dim, model_dim)) #torch.randn((model_dim, model_dim), dtype=torch.float32) * 0.01\n",
    "        self.parameters[\"W1\"] = self.ff_layer\n",
    "        self.ff_layer2 = self.init_weights((model_dim, model_dim)) #torch.randn((model_dim, model_dim), dtype=torch.float32) * 0.01\n",
    "        self.parameters[\"W2\"] = self.ff_layer2\n",
    "        self.ff_bias = torch.randn((1, model_dim), dtype=torch.float32)\n",
    "        self.parameters[\"b1\"] = self.ff_bias\n",
    "        self.ff_bias2 = torch.randn((1, model_dim), dtype=torch.float32)\n",
    "        self.parameters[\"b2\"] = self.ff_bias2\n",
    "        self.output = torch.randn((model_dim, vocab_size), dtype=torch.float32)\n",
    "\n",
    "    def positional_encoding(self, x_pos):\n",
    "        \"\"\"sinus non-trainable positional encoding to add with input\"\"\"\n",
    "        batch_size, seq_len = x_pos.shape\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.model_dim, 2, dtype=torch.float32) * (-torch.log(torch.tensor(10000.0)) / self.model_dim)\n",
    "        )\n",
    "\n",
    "        pe = torch.zeros(batch_size, seq_len, self.model_dim, dtype=torch.float32)\n",
    "\n",
    "        pe[:, :, 0::2] = torch.sin(x_pos.unsqueeze(-1) * div_term)  # sin for even indices\n",
    "        pe[:, :, 1::2] = torch.cos(x_pos.unsqueeze(-1) * div_term)  # cos for odd indices\n",
    "        return pe * 0.005\n",
    "    \n",
    "    def residual_connexion(self, current_layer, dragged_layer):\n",
    "        \"\"\"generic residual connexion operation\"\"\"\n",
    "        return current_layer + dragged_layer\n",
    "\n",
    "    def input_block(self, x, x_pos):\n",
    "        \"\"\"embedding input and positional encoding input\"\"\"\n",
    "        input_embed = self.embedding[x]\n",
    "        position_embed = self.positional_encoding(x_pos)\n",
    "        self.position_embed_norm = torch.norm(position_embed)\n",
    "        # print(f\"Positional Encoding Magnitude: {torch.norm(position_embed)}\")\n",
    "        total_input = input_embed + position_embed\n",
    "        self.cache[\"input_block_input\"] = total_input\n",
    "        total_input, total_input_mean, total_input_std = self.layernorm(total_input)\n",
    "        self.cache[\"input_block_normalized_input\"] = total_input\n",
    "        self.cache[\"input_block_mean\"] = total_input_mean\n",
    "        self.cache[\"input_block_std\"] = total_input_std\n",
    "        total_input = self.dropout(total_input, 0.2)\n",
    "        return total_input\n",
    "    \n",
    "    def attention_block(self, x):\n",
    "        \"\"\"full attention block, batch-compatible\"\"\"\n",
    "        all_attn_outputs = []\n",
    "        batch_size, seq_len, model_dim = x.shape\n",
    "        head_size = model_dim // self.nb_heads\n",
    "\n",
    "        self.cache[\"attn_X\"] = x\n",
    "\n",
    "        # Create lower triangular mask once and broadcast across batch\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.float32))\n",
    "        mask = mask.unsqueeze(0).expand(batch_size, -1, -1)  # [batch_size, seq_len, seq_len]\n",
    "\n",
    "        for n in range(self.nb_heads):\n",
    "            # Project input to K, Q, V for this head\n",
    "            K = torch.einsum(\"bsm,mh->bsh\", x, self.attn_projs[f\"k_proj_{n}\"])\n",
    "            Q = torch.einsum(\"bsm,mh->bsh\", x, self.attn_projs[f\"q_proj_{n}\"])\n",
    "            V = torch.einsum(\"bsm,mh->bsh\", x, self.attn_projs[f\"v_proj_{n}\"])\n",
    "\n",
    "            self.cache[f\"K{n}\"] = K\n",
    "            self.cache[f\"Q{n}\"] = Q\n",
    "            self.cache[f\"V{n}\"] = V\n",
    "\n",
    "            # Compute attention scores: Q @ K.T (batched attention matmul)\n",
    "            pre_attention = torch.einsum(\"bqh,bkh->bqk\", Q, K)  # [batch, seq, seq]\n",
    "\n",
    "            # Scale\n",
    "            scale = pre_attention / torch.sqrt(torch.tensor(head_size, dtype=torch.float32))\n",
    "\n",
    "            # Apply mask and handle -inf for softmax\n",
    "            scale = scale.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "            # Softmax over the last dimension (attention scores over the sequence)\n",
    "            attn_softm = torch.softmax(scale, dim=-1)\n",
    "\n",
    "            attn_softm = self.dropout(attn_softm, 0.2)\n",
    "            self.cache[f\"attn_probs{n}\"] = attn_softm\n",
    "\n",
    "            # Compute attention output (weighted sum of values)\n",
    "            attn_output = torch.einsum(\"bqk,bkh->bqh\", attn_softm, V)  # [batch, seq, head_size]\n",
    "            all_attn_outputs.append(attn_output)\n",
    "\n",
    "        # Concatenate all head outputs along last dimension (feature dimension)\n",
    "        all_attn_outputs = torch.cat(all_attn_outputs, dim=-1)  # [batch, seq, model_dim]\n",
    "\n",
    "        # Final projection back to model dimension\n",
    "        all_attn_outputs = torch.einsum(\"bsm,mh->bsh\", all_attn_outputs, self.attn_proj_back)\n",
    "\n",
    "        all_attn_outputs = self.dropout(all_attn_outputs, 0.2)\n",
    "        return all_attn_outputs\n",
    "\n",
    "    def linear_block(self, x):\n",
    "        \"\"\"linear layer block before output\"\"\"\n",
    "        self.cache[\"linear_block_input\"] = x\n",
    "        x, x_mean, x_std = self.layernorm(x)\n",
    "        self.cache[\"linear_block_normalized_input\"] = x\n",
    "        self.cache[\"linear_block_mean\"] = x_mean\n",
    "        self.cache[\"linear_block_std\"] = x_std\n",
    "        self.activations[\"A0\"] = x\n",
    "        ff_outout = x @ self.ff_layer + self.ff_bias # a0 * W1 + b1\n",
    "        ff_outout = self.dropout(ff_outout, 0.2) # \n",
    "        ff_activation = self.relu_activation(ff_outout) # a1 = activation(W1 * x + b1)\n",
    "        self.activations[\"A1\"] = ff_activation\n",
    "        ff_output2 = ff_activation @ self.ff_layer2 + self.ff_bias2 # a1 * W2  + b2\n",
    "        logits = ff_output2 @ self.embedding.T\n",
    "        return logits\n",
    "    \n",
    "    def dropout(self, x, p):\n",
    "        mask = (torch.rand_like(x) > p).float()\n",
    "        return x * mask\n",
    "    \n",
    "    def init_weights(self, shape):\n",
    "        return torch.randn(shape, dtype=torch.float32) * 0.01\n",
    "\n",
    "    def layernorm(self, x):\n",
    "        \"\"\"generic normalization layer\"\"\"\n",
    "        x_mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        x_std = torch.std(x, dim=-1, keepdim=True)\n",
    "        x_std = torch.clamp(x_std, min=1e-6)\n",
    "        return (x - x_mean) / x_std, x_mean, x_std\n",
    "    \n",
    "    def relu_activation(self, x):\n",
    "        \"\"\"generic rectified linear unit layer\"\"\"\n",
    "        return torch.relu(x)\n",
    "\n",
    "    def forward(self, x, x_idx):\n",
    "        \"\"\"full transformer forward pass\"\"\"\n",
    "        total_input = self.input_block(x, x_idx)\n",
    "        attention = self.attention_block(total_input)\n",
    "        residual1 = self.residual_connexion(attention, total_input * (0.5**0.5))\n",
    "        logits = self.linear_block(residual1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, y):\n",
    "    log_softmax = torch.log_softmax(logits, dim=-1)\n",
    "    log_softmax_mean = log_softmax.mean()\n",
    "    log_softmax -= log_softmax_mean\n",
    "    target_log_probs = log_softmax.gather(2, y.unsqueeze(-1)).squeeze(-1)\n",
    "    loss = -target_log_probs.mean()\n",
    "    return log_softmax, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layernorm_backprop(dout, x, mean, std):\n",
    "\n",
    "    xmu = x - mean\n",
    "    xsgma = 1 / (std + 1e-6)\n",
    "    dnorm = dout * xsgma\n",
    "\n",
    "    dx = dnorm\n",
    "    dx -= dnorm.mean(dim=1, keepdims=True)\n",
    "    dx -= (xmu / (std + 1e-6) ** 2) * (dnorm * xmu).mean(dim=1, keepdims=True)\n",
    "    return dx\n",
    "\n",
    "def full_backprop(x, y, output_softmax, parameters, activations, cache, attn_projs, attn_proj_back):\n",
    "    grads = {}\n",
    "\n",
    "    batch_size, seq_len = x.shape\n",
    "\n",
    "    # ======= Linear Block Backprop (Logits to FFN) =======\n",
    "\n",
    "    dZ2 = output_softmax.clone()\n",
    "    dZ2[torch.arange(batch_size).unsqueeze(1), torch.arange(seq_len), y] -= 1\n",
    "    dZ2 /= (batch_size * seq_len)  # Normalize across batch and sequence\n",
    "\n",
    "    # Final projection (logits = ff_output2 @ embedding.T)\n",
    "    grads[\"dEmbedding\"] = torch.einsum(\"bsi,bsj->ij\", dZ2, activations[\"A1\"])\n",
    "\n",
    "    # Backprop through FFN (W2, b2, ReLU, W1, b1)\n",
    "    dA1 = torch.einsum(\"bsi,ij->bsj\", dZ2, parameters[\"embedding\"])\n",
    "\n",
    "    grads[\"dW2\"] = torch.einsum(\"bsi,bsj->ij\", activations[\"A1\"], dA1)\n",
    "    grads[\"db2\"] = dA1.sum(dim=(0, 1), keepdim=False).unsqueeze(0)\n",
    "\n",
    "    dZ1 = dA1 * (activations[\"A0\"] > 0).float()\n",
    "\n",
    "    grads[\"dW1\"] = torch.einsum(\"bsi,bsj->ij\", activations[\"A0\"], dZ1)\n",
    "    grads[\"db1\"] = dZ1.sum(dim=(0, 1), keepdim=False).unsqueeze(0)\n",
    "\n",
    "    dA0 = torch.einsum(\"bsj,ij->bsi\", dZ1, parameters[\"W1\"])\n",
    "\n",
    "    # Backprop through layernorm after attention block\n",
    "    grads[\"dA0\"] = layernorm_backprop(\n",
    "        dA0,\n",
    "        cache[\"linear_block_input\"],\n",
    "        cache[\"linear_block_mean\"],\n",
    "        cache[\"linear_block_std\"],\n",
    "    )\n",
    "\n",
    "    # ======= Attention Block Backprop (Multi-head Attention) =======\n",
    "\n",
    "    head_size = cache[\"model_dim\"] // cache[\"nb_heads\"]\n",
    "\n",
    "    # Fix: Project back and reshape into heads directly\n",
    "    combined_heads = torch.einsum(\"bsi,ij->bsj\", grads[\"dA0\"], attn_proj_back)\n",
    "    dHeads = combined_heads.view(batch_size, seq_len, cache[\"nb_heads\"], head_size).unbind(dim=2)\n",
    "\n",
    "    dX = torch.zeros_like(cache[\"attn_X\"])\n",
    "\n",
    "    for h in range(cache[\"nb_heads\"]):\n",
    "        Q = cache[f\"Q{h}\"]\n",
    "        K = cache[f\"K{h}\"]\n",
    "        V = cache[f\"V{h}\"]\n",
    "        attn_probs = cache[f\"attn_probs{h}\"]\n",
    "\n",
    "        dHead = dHeads[h]  # [batch, seq, head_size]\n",
    "\n",
    "        # Backprop into V\n",
    "        dV = torch.einsum(\"bqk,bqh->bkh\", attn_probs, dHead)\n",
    "        grads[f\"dW_v{h}\"] = torch.einsum(\"bsm,bsh->mh\", cache[\"attn_X\"], dV)\n",
    "\n",
    "        # Backprop into attention scores\n",
    "        dAttn = torch.einsum(\"bqh,bkh->bqk\", dHead, V)\n",
    "\n",
    "        # Backprop through softmax (Jacobian trick)\n",
    "        dScores = attn_probs * (dAttn - (dAttn * attn_probs).sum(dim=-1, keepdim=True))\n",
    "\n",
    "        # Backprop into Q and K\n",
    "        dQ = torch.einsum(\"bqk,bkh->bqh\", dScores, K)\n",
    "        dK = torch.einsum(\"bqk,bqh->bkh\", dScores, Q)\n",
    "\n",
    "        grads[f\"dW_q{h}\"] = torch.einsum(\"bsm,bsh->mh\", cache[\"attn_X\"], dQ)\n",
    "        grads[f\"dW_k{h}\"] = torch.einsum(\"bsm,bsh->mh\", cache[\"attn_X\"], dK)\n",
    "\n",
    "        # Backprop into input X\n",
    "        dX += (\n",
    "            torch.einsum(\"bsh,mh->bsm\", dQ, attn_projs[f\"q_proj_{h}\"]) +\n",
    "            torch.einsum(\"bsh,mh->bsm\", dK, attn_projs[f\"k_proj_{h}\"]) +\n",
    "            torch.einsum(\"bsh,mh->bsm\", dV, attn_projs[f\"v_proj_{h}\"])\n",
    "        )\n",
    "\n",
    "    # ======= Residual Connection (Add attention & FFN gradients) =======\n",
    "    dResidual = grads[\"dA0\"] + dX\n",
    "\n",
    "    # ======= Input Block Backprop (Embedding + Positional Encoding + Layernorm) =======\n",
    "    dResidual /= (1 - cache[\"dropout_p\"])  # Backprop through dropout\n",
    "\n",
    "    assert dResidual.shape == dA0.shape\n",
    "    \n",
    "    dTotalInput = layernorm_backprop(\n",
    "        dResidual,\n",
    "        cache[\"input_block_input\"],\n",
    "        cache[\"input_block_mean\"],\n",
    "        cache[\"input_block_std\"],\n",
    "    )\n",
    "\n",
    "    dInputEmbed = dTotalInput.clone()\n",
    "\n",
    "    # Combine gradients into the embedding table\n",
    "    if \"dEmbedding\" not in grads:\n",
    "        grads[\"dEmbedding\"] = torch.zeros_like(parameters[\"embedding\"])\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for t in range(seq_len):\n",
    "            token_id = x[b, t]\n",
    "            grads[\"dEmbedding\"][token_id] += dInputEmbed[b, t]\n",
    "    \n",
    "    # Optional positional encoding backprop (unused unless trainable)\n",
    "    grads[\"dPositionalEncoding\"] = dTotalInput.clone()\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_step(grads, parameters, optim_states, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.001):\n",
    "    \n",
    "    m = optim_states[\"m\"]\n",
    "    v = optim_states[\"v\"]\n",
    "    t = optim_states[\"t\"]\n",
    "\n",
    "    # Map parameters to their corresponding gradient names\n",
    "    param_grad_map = {\n",
    "        \"W1\": \"dW1\",\n",
    "        \"W2\": \"dW2\",\n",
    "        \"b1\": \"db1\",\n",
    "        \"b2\": \"db2\",\n",
    "        \"embedding\": \"dEmbedding\"\n",
    "    }\n",
    "\n",
    "    for h in range(parameters[\"nb_heads\"]):  # Use passed-in `nb_heads`, don't read from parameters\n",
    "        param_grad_map[f\"q_proj_{h}\"] = f\"dW_q{h}\"\n",
    "        param_grad_map[f\"k_proj_{h}\"] = f\"dW_k{h}\"\n",
    "        param_grad_map[f\"v_proj_{h}\"] = f\"dW_v{h}\"\n",
    "\n",
    "    for param_name, grad_name in param_grad_map.items():\n",
    "        if param_name not in parameters or grad_name not in grads:\n",
    "            continue\n",
    "\n",
    "        param = parameters[param_name]\n",
    "        grad = grads[grad_name]\n",
    "\n",
    "        if param_name not in m:\n",
    "            m[param_name] = torch.zeros_like(param)\n",
    "            v[param_name] = torch.zeros_like(param)\n",
    "\n",
    "        # AdamW moment updates\n",
    "        m[param_name] = beta1 * m[param_name] + (1 - beta1) * grad\n",
    "        v[param_name] = beta2 * v[param_name] + (1 - beta2) * (grad ** 2)\n",
    "\n",
    "        # Bias correction\n",
    "        m_hat = m[param_name] / (1 - beta1 ** t)\n",
    "        v_hat = v[param_name] / (1 - beta2 ** t)\n",
    "\n",
    "        # Weight update (decoupled weight decay for non-embedding params)\n",
    "        param_update = lr * m_hat / (torch.sqrt(v_hat) + eps)\n",
    "\n",
    "        if param_name in [\"embedding\", \"b1\", \"b2\"]:\n",
    "            param -= param_update  # No weight decay on embeddings\n",
    "        else:\n",
    "            param -= param_update + weight_decay * param  # AdamW-style weight decay\n",
    "\n",
    "        parameters[param_name] = param  # Update parameter in-place\n",
    "\n",
    "    t += 1\n",
    "\n",
    "    optim_states[\"m\"] = m\n",
    "    optim_states[\"v\"] = v\n",
    "    optim_states[\"t\"] = t\n",
    "\n",
    "    return parameters, optim_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_grad_norm(grads):\n",
    "    \"\"\"Computes the global gradient norm across all parameters.\"\"\"\n",
    "    total_norm = 0.0\n",
    "    for g in grads.values():\n",
    "        if g is not None:\n",
    "            total_norm += g.norm().item() ** 2\n",
    "    return total_norm ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipping(grads, max_norm=1.0):\n",
    "    \"\"\"Apply gradient clipping to all gradients.\"\"\"\n",
    "    total_norm = global_grad_norm(grads)\n",
    "\n",
    "    if math.isnan(total_norm) or math.isinf(total_norm):  # <- Use math instead of torch\n",
    "        raise ValueError(f\"NaN or Inf detected in global grad norm!\")\n",
    "\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1.0:\n",
    "        for g in grads.values():\n",
    "            g *= clip_coef\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nan_inf(grads):\n",
    "    \"\"\"Check for NaN/Inf gradients in any parameter.\"\"\"\n",
    "    for name, grad in grads.items():\n",
    "        if torch.isnan(grad).any() or torch.isinf(grad).any():\n",
    "            print(f\"ðŸš¨ Gradient explosion detected in {name}\")\n",
    "            raise ValueError(f\"Gradient in {name} contains NaN/Inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim, nb_heads = 64, 1\n",
    "transformer = BasicTransformerNetwork(vocab_size=len(vocab), model_dim=model_dim, nb_heads=nb_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"This is an example sentence\"\n",
    "input_ids = encode(example)\n",
    "pos_ids = list(range(len(input_ids)))\n",
    "\n",
    "input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "pos_ids = torch.tensor(pos_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = transformer.forward(input_ids, pos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randint(0, 63, (1, 27), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax, loss = compute_loss(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = full_backprop(\n",
    "    input_ids,\n",
    "    y,\n",
    "    softmax,\n",
    "    transformer.parameters,\n",
    "    transformer.activations,\n",
    "    transformer.cache,\n",
    "    transformer.attn_projs,\n",
    "    transformer.attn_proj_back,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.parameters, _ = optimizer_step(grads, transformer.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.8865989446640015 to 0.8521111607551575\n",
      "Softmax Range: -0.8887062072753906 to 0.8502516746520996\n",
      "Iteration 0: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.06995103508234024\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.8629919290542603 to 0.8388312458992004\n",
      "Softmax Range: -0.8642644882202148 to 0.8376955986022949\n",
      "Iteration 200: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.06415937840938568\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.853061854839325 to 0.8305718302726746\n",
      "Softmax Range: -0.8548002243041992 to 0.8290166854858398\n",
      "Iteration 400: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.05233614146709442\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.8379678130149841 to 0.8167802095413208\n",
      "Softmax Range: -0.839900016784668 to 0.8152532577514648\n",
      "Iteration 600: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.05499274656176567\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.8285660743713379 to 0.8098031282424927\n",
      "Softmax Range: -0.830296516418457 to 0.8078470230102539\n",
      "Iteration 800: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.04985669627785683\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.824856698513031 to 0.8026825189590454\n",
      "Softmax Range: -0.8268747329711914 to 0.8006801605224609\n",
      "Iteration 1000: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.051447294652462006\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.8153204321861267 to 0.7946911454200745\n",
      "Softmax Range: -0.817418098449707 to 0.7928409576416016\n",
      "Iteration 1200: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.05121230706572533\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.8111191987991333 to 0.7911050915718079\n",
      "Softmax Range: -0.8131504058837891 to 0.7886276245117188\n",
      "Iteration 1400: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.03783607482910156\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.809965968132019 to 0.790214478969574\n",
      "Softmax Range: -0.8123502731323242 to 0.7879452705383301\n",
      "Iteration 1600: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.037718236446380615\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.8051924705505371 to 0.7882356643676758\n",
      "Softmax Range: -0.8078575134277344 to 0.7855701446533203\n",
      "Iteration 1800: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.04466140270233154\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.804966926574707 to 0.7874196767807007\n",
      "Softmax Range: -0.8075351715087891 to 0.7846155166625977\n",
      "Iteration 2000: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.03651546686887741\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7998119592666626 to 0.7845802307128906\n",
      "Softmax Range: -0.8025894165039062 to 0.7816596031188965\n",
      "Iteration 2200: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.05099601298570633\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7951307892799377 to 0.7835673689842224\n",
      "Softmax Range: -0.7979507446289062 to 0.7807478904724121\n",
      "Iteration 2400: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.035690076649188995\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7954776287078857 to 0.7828966975212097\n",
      "Softmax Range: -0.7984256744384766 to 0.779932975769043\n",
      "Iteration 2600: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.039354272186756134\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7919528484344482 to 0.7832628488540649\n",
      "Softmax Range: -0.7951030731201172 to 0.7801351547241211\n",
      "Iteration 2800: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.036767520010471344\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7909654378890991 to 0.7826194167137146\n",
      "Softmax Range: -0.7942132949829102 to 0.7793717384338379\n",
      "Iteration 3000: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.026157012209296227\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7922805547714233 to 0.7838234305381775\n",
      "Softmax Range: -0.7958822250366211 to 0.7802219390869141\n",
      "Iteration 3200: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.0358191542327404\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7888343334197998 to 0.7823493480682373\n",
      "Softmax Range: -0.7923583984375 to 0.7790017127990723\n",
      "Iteration 3400: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.03287023678421974\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.78458571434021 to 0.7796381711959839\n",
      "Softmax Range: -0.7879505157470703 to 0.7762737274169922\n",
      "Iteration 3600: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.032678812742233276\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7827135920524597 to 0.778927206993103\n",
      "Softmax Range: -0.7860183715820312 to 0.7756228446960449\n",
      "Iteration 3800: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.039996352046728134\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7827523350715637 to 0.7789192795753479\n",
      "Softmax Range: -0.7862224578857422 to 0.775385856628418\n",
      "Iteration 4000: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.03382682427763939\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7805866599082947 to 0.7771326303482056\n",
      "Softmax Range: -0.7840557098388672 to 0.7736635208129883\n",
      "Iteration 4200: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.020743826404213905\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7851037383079529 to 0.7808555364608765\n",
      "Softmax Range: -0.7891569137573242 to 0.7768025398254395\n",
      "Iteration 4400: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.026845311746001244\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7798126935958862 to 0.776199996471405\n",
      "Softmax Range: -0.7834692001342773 to 0.7725429534912109\n",
      "Iteration 4600: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.01881502941250801\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7790558934211731 to 0.7749339938163757\n",
      "Softmax Range: -0.7827892303466797 to 0.7712512016296387\n",
      "Iteration 4800: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.02605529874563217\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7770227789878845 to 0.7730122208595276\n",
      "Softmax Range: -0.7806892395019531 to 0.769345760345459\n",
      "Iteration 5000: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.01630404219031334\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7772634029388428 to 0.7728246450424194\n",
      "Softmax Range: -0.7810382843017578 to 0.7690496444702148\n",
      "Iteration 5200: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.01437416486442089\n",
      "\n",
      "Positional Encoding Magnitude: 2.479029655456543\n",
      "Logits Range: -0.7771792411804199 to 0.7723186016082764\n",
      "Softmax Range: -0.7810678482055664 to 0.7684297561645508\n",
      "Iteration 5400: Global Grad Norm = 1.0000\n",
      "\n",
      "training loss:  0.02154463715851307\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[176]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m softmax, loss = compute_loss(logits, batch_outputs)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# print(f\"Softmax Range: {softmax.min()} to {softmax.max()}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m grads = \u001b[43mfull_backprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn_projs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn_proj_back\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Check for NaN/Inf in grads\u001b[39;00m\n\u001b[32m     54\u001b[39m check_for_nan_inf(grads)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[170]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mfull_backprop\u001b[39m\u001b[34m(x, y, output_softmax, parameters, activations, cache, attn_projs, attn_proj_back)\u001b[39m\n\u001b[32m     25\u001b[39m grads[\u001b[33m\"\u001b[39m\u001b[33mdEmbedding\u001b[39m\u001b[33m\"\u001b[39m] = torch.einsum(\u001b[33m\"\u001b[39m\u001b[33mbsi,bsj->ij\u001b[39m\u001b[33m\"\u001b[39m, dZ2, activations[\u001b[33m\"\u001b[39m\u001b[33mA1\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Backprop through FFN (W2, b2, ReLU, W1, b1)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m dA1 = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbsi,ij->bsj\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdZ2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membedding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m grads[\u001b[33m\"\u001b[39m\u001b[33mdW2\u001b[39m\u001b[33m\"\u001b[39m] = torch.einsum(\u001b[33m\"\u001b[39m\u001b[33mbsi,bsj->ij\u001b[39m\u001b[33m\"\u001b[39m, activations[\u001b[33m\"\u001b[39m\u001b[33mA1\u001b[39m\u001b[33m\"\u001b[39m], dA1)\n\u001b[32m     31\u001b[39m grads[\u001b[33m\"\u001b[39m\u001b[33mdb2\u001b[39m\u001b[33m\"\u001b[39m] = dA1.sum(dim=(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m), keepdim=\u001b[38;5;28;01mFalse\u001b[39;00m).unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/dl_env/lib/python3.13/site-packages/torch/functional.py:407\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, *_operands)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    406\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    409\u001b[39m path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum.is_available():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "max_window = 30\n",
    "batch_size = 32\n",
    "nb_epochs = 5\n",
    "optim_states = {\n",
    "    \"m\": {},\n",
    "    \"v\": {},\n",
    "    \"t\": 1,\n",
    "}\n",
    "model_dim, nb_heads = 512, 4\n",
    "transformer = BasicTransformerNetwork(vocab_size=len(vocab), model_dim=model_dim, nb_heads=nb_heads)\n",
    "\n",
    "for iter in range(len(text_dataset) // batch_size):\n",
    "    \n",
    "    batch_inputs = []\n",
    "    batch_pos = []\n",
    "    batch_outputs = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "\n",
    "        start_idx = int(torch.randint(0, len(text_dataset) - max_window - 1, (1,)))\n",
    "        input_text = text_dataset[start_idx:start_idx + max_window] # x\n",
    "        output_text = text_dataset[start_idx+1:start_idx + max_window + 1] # y\n",
    "        input_ids = encode(input_text)\n",
    "        pos_ids = list(range(len(input_ids)))\n",
    "        output_ids = encode(output_text)\n",
    "\n",
    "        batch_inputs.append(input_ids)\n",
    "        batch_pos.append(pos_ids)\n",
    "        batch_outputs.append(output_ids)\n",
    "\n",
    "    batch_inputs = torch.tensor(batch_inputs, dtype=torch.long)  # [batch_size, seq_len]\n",
    "    batch_pos = torch.tensor(batch_pos, dtype=torch.long)\n",
    "    batch_outputs = torch.tensor(batch_outputs, dtype=torch.long)\n",
    "\n",
    "    logits = transformer.forward(batch_inputs, batch_pos)\n",
    "\n",
    "    logits = torch.clamp(logits, -5, 5)\n",
    "    softmax, loss = compute_loss(logits, batch_outputs)\n",
    "\n",
    "    grads = full_backprop(\n",
    "        batch_inputs,\n",
    "        batch_outputs,\n",
    "        softmax,\n",
    "        transformer.parameters,\n",
    "        transformer.activations,\n",
    "        transformer.cache,\n",
    "        transformer.attn_projs,\n",
    "        transformer.attn_proj_back,\n",
    "    )\n",
    "\n",
    "    check_for_nan_inf(grads)\n",
    "\n",
    "    # Clip gradients\n",
    "    grads = gradient_clipping(grads, max_norm=1.0)\n",
    "\n",
    "    # Log global gradient norm\n",
    "    global_norm = global_grad_norm(grads)\n",
    "\n",
    "    transformer.parameters, optim_states = optimizer_step(\n",
    "        grads, transformer.parameters, optim_states, lr=5e-7\n",
    "    )\n",
    "\n",
    "    if not iter % 200:\n",
    "        print(f\"Positional Encoding Magnitude: {transformer.position_embed_norm}\")\n",
    "        print(f\"Logits Range: {logits.min()} to {logits.max()}\")\n",
    "        print(f\"Softmax Range: {softmax.min()} to {softmax.max()}\")\n",
    "        print(f\"Iteration {iter}: Global Grad Norm = {global_norm:.4f}\")\n",
    "        print()\n",
    "        print(\"training loss: \", float(loss))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
